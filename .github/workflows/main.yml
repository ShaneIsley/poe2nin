# .github/workflows/main.yml

name: Fetch PoE2 Economy Data

on:
  workflow_dispatch: # Allows you to run the job manually from the Actions tab
  schedule:
    # Runs regularly.
    - cron: "*/20 * * * *"

jobs:
  fetch-and-commit:
    runs-on: ubuntu-latest

    # --- FIX: Add this permissions block ---
    # This grants the GITHUB_TOKEN the permissions to write to the repository.
    permissions:
      contents: write

    steps:
      # Step 1: Check out your repository's code
      - name: Checkout Repo
        uses: actions/checkout@v4

      # Step 2: Set up the Python version
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      # Step 3: Install the Python libraries
      - name: Install dependencies
        run: pip install -r requirements.txt

      # Step 4: Run the Python script to update the DB
      - name: Run Fetch Data Script
        run: python fetch_data.py

      - name: Debug Data Collection (fetch_data.py)
        run: |
          echo "--- Starting Scraper Debug Step for fetch_data.py ---"
          DB_FILE="poe2_economy.db"
          # Define paths based on the script's configuration
          LEAGUE_DIR="data/rise_of_the_abyssal"
          CURRENCY_FILE="$LEAGUE_DIR/currency.json"

          echo "[1] Checking Divine Orb timestamp BEFORE running scraper..."
          # Use || true to prevent the workflow from failing if the timestamp doesn't exist yet
          TIMESTAMP_BEFORE=$(sqlite3 $DB_FILE "SELECT MAX(p.timestamp) FROM items i JOIN price_entries p ON i.id = p.item_id WHERE i.name = 'Divine Orb';" || true)
          echo "Timestamp Before: ${TIMESTAMP_BEFORE:-'Not Found'}"

          echo "[2] Removing old currency.json to ensure a clean test run..."
          rm -f "$CURRENCY_FILE"

          echo "[3] Running fetch_data.py to collect new data..."
          # The script has built-in logging, which will be displayed here.
          # We expect to see output for the 'Currency' category.
          python fetch_data.py

          echo "[4] Verifying that 'currency.json' was created and contains data..."
          if [ -f "$CURRENCY_FILE" ]; then
            echo "  - SUCCESS: '$CURRENCY_FILE' was created."
            echo "  - Checking for 'Divine Orb' entry in the file..."
            if grep -q -i "Divine Orb" "$CURRENCY_FILE"; then
              echo "    - SUCCESS: Found 'Divine Orb' in the downloaded data."
            else
              echo "    - CRITICAL FAILURE: 'Divine Orb' is MISSING from the API response in '$CURRENCY_FILE'."
            fi
          else
            echo "  - CRITICAL FAILURE: '$CURRENCY_FILE' was NOT created. Check the script output in step [3] for API or network errors."
          fi

          echo "[5] Checking Divine Orb timestamp AFTER running scraper..."
          TIMESTAMP_AFTER=$(sqlite3 $DB_FILE "SELECT MAX(p.timestamp) FROM items i JOIN price_entries p ON i.id = p.item_id WHERE i.name = 'Divine Orb';" || true)
          echo "Timestamp After:  ${TIMESTAMP_AFTER:-'Not Found'}"

          echo "[6] --- CONCLUSION ---"
          if [[ "$TIMESTAMP_BEFORE" == "$TIMESTAMP_AFTER" ]]; then
            echo "RESULT: FAILED. The timestamp for Divine Orb did NOT change."
            echo "ACTION: The issue is with the scraper (fetch_data.py) or the poe.ninja API."
            echo "        Review the log from step [3] and [4] above for specific error messages."
            exit 1 # Explicitly fail the step to make the error obvious
          else
            echo "RESULT: SUCCESS! The timestamp for Divine Orb was updated by the scraper."
          fi
          echo "----------------------------------------------------"

      - name: Debug Database State Before Analysis
        run: |
          DB_FILE="poe2_economy.db"
          echo "--- Verifying Database File ---"
          ls -lh $DB_FILE

          echo "--- Checking Overall Most Recent Timestamp in DB ---"
          sqlite3 $DB_FILE "SELECT MAX(timestamp) FROM price_entries;"

          echo "--- Checking Most Recent Timestamps for Key Currencies (ALL TIME) ---"
          sqlite3 $DB_FILE "SELECT i.name, MAX(p.timestamp) AS most_recent_update FROM items i JOIN price_entries p ON i.id = p.item_id WHERE i.name IN ('Divine Orb', 'Exalted Orb', 'Chaos Orb') GROUP BY i.name;"

          echo "--- Checking for Key Currencies in the LAST 2 DAYS ---"
          echo "(This is the exact data the Python script should be receiving)"
          sqlite3 $DB_FILE "SELECT i.name, COUNT(*) as entry_count, MAX(p.timestamp) AS latest_update FROM items i JOIN price_entries p ON i.id = p.item_id WHERE i.name IN ('Divine Orb', 'Exalted Orb', 'Chaos Orb') AND p.timestamp >= DATETIME('now', '-2 days') GROUP BY i.name;"

          echo "--- Verifying Script Content ---"
          cat analysis.py

      - name: Run Analysis Script
        run: python analysis.py

      # Step 5: Commit the updated database file
      - name: Commit and push if changed
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          # Add the database, the README, and the new charts folder
          git add *.db README.md charts/ data/
          git status
          git diff --staged --quiet || git commit -m "Update economy data and analysis"
          git push
