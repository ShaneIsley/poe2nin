# .github/workflows/main.yml

name: Fetch PoE2 Economy Data

on:
  workflow_dispatch: # Allows you to run the job manually from the Actions tab
  schedule:
    # Runs regularly.
    - cron: "*/20 * * * *"

jobs:
  fetch-and-commit:
    runs-on: ubuntu-latest

    # --- FIX: Add this permissions block ---
    # This grants the GITHUB_TOKEN the permissions to write to the repository.
    permissions:
      contents: write

    steps:
      # Step 1: Check out your repository's code
      - name: Checkout Repo
        uses: actions/checkout@v4

      # Step 2: Set up the Python version
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      # Step 3: Install the Python libraries
      - name: Install dependencies
        run: pip install -r requirements.txt

      # Step 4: Run the Python script to update the DB
      - name: Run Fetch Data Script
        run: python fetch_data.py

      - name: Debug Data Collection for Divine Orb
        run: |
          echo "--- Starting Scraper Debug Step ---"
          DB_FILE="poe2_economy.db"

          echo "[1] Checking Divine Orb timestamp BEFORE running scraper..."
          TIMESTAMP_BEFORE=$(sqlite3 $DB_FILE "SELECT MAX(p.timestamp) FROM items i JOIN price_entries p ON i.id = p.item_id WHERE i.name = 'Divine Orb';")
          echo "Timestamp Before: $TIMESTAMP_BEFORE"

          echo "[2] Testing connectivity to data source (poe.ninja)..."
          # This curl command attempts to fetch the raw data for currency.
          # A 200 OK status means the runner can reach the server.
          curl -I "https://poe.ninja/api/data/currencyoverview?league=Necropolis&type=Currency"

          echo "[3] Attempting to run scraper for 'Divine Orb' ONLY..."
          # !!! IMPORTANT !!!
          # REPLACE the command below with the actual command to run your scraper.
          # If your scraper has a way to target a single item, use that.
          # If not, run the full scraper and we will check its output.
          # Example for a full run: python3 src/scraper.py
          # Example for a targeted run: python3 src/scraper.py --item "Divine Orb"
          
          python your_scraper_script.py

          echo "[4] Checking Divine Orb timestamp AFTER running scraper..."
          TIMESTAMP_AFTER=$(sqlite3 $DB_FILE "SELECT MAX(p.timestamp) FROM items i JOIN price_entries p ON i.id = p.item_id WHERE i.name = 'Divine Orb';")
          echo "Timestamp After:  $TIMESTAMP_AFTER"

          echo "[5] --- CONCLUSION ---"
          if [[ "$TIMESTAMP_BEFORE" == "$TIMESTAMP_AFTER" ]]; then
            echo "RESULT: FAILED. The timestamp for Divine Orb did NOT change."
            echo "ACTION: Check the output from step [3] for error messages from your scraper."
            echo "Common issues: silent HTTP errors, incorrect item ID/name, JSON parsing errors."
          else
            echo "RESULT: SUCCESS! The timestamp for Divine Orb was updated."
          fi
          echo "------------------------"

      - name: Debug Database State Before Analysis
        run: |
          DB_FILE="poe2_economy.db"
          echo "--- Verifying Database File ---"
          ls -lh $DB_FILE

          echo "--- Checking Overall Most Recent Timestamp in DB ---"
          sqlite3 $DB_FILE "SELECT MAX(timestamp) FROM price_entries;"

          echo "--- Checking Most Recent Timestamps for Key Currencies (ALL TIME) ---"
          sqlite3 $DB_FILE "SELECT i.name, MAX(p.timestamp) AS most_recent_update FROM items i JOIN price_entries p ON i.id = p.item_id WHERE i.name IN ('Divine Orb', 'Exalted Orb', 'Chaos Orb') GROUP BY i.name;"

          echo "--- Checking for Key Currencies in the LAST 2 DAYS ---"
          echo "(This is the exact data the Python script should be receiving)"
          sqlite3 $DB_FILE "SELECT i.name, COUNT(*) as entry_count, MAX(p.timestamp) AS latest_update FROM items i JOIN price_entries p ON i.id = p.item_id WHERE i.name IN ('Divine Orb', 'Exalted Orb', 'Chaos Orb') AND p.timestamp >= DATETIME('now', '-2 days') GROUP BY i.name;"

          echo "--- Verifying Script Content ---"
          cat analysis.py

      - name: Run Analysis Script
        run: python analysis.py

      # Step 5: Commit the updated database file
      - name: Commit and push if changed
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          # Add the database, the README, and the new charts folder
          git add *.db README.md charts/ data/
          git status
          git diff --staged --quiet || git commit -m "Update economy data and analysis"
          git push
